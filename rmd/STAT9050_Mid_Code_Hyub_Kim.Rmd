---
title: "STAT9050_Mid_Code_HyubKim"
author: "김협"
output: html_document
---

# STAT9050-01. 응용통계학 특수문제 I

## Part 1. 

### (a) Survival Times Simulation and Cox Proportional Hazards Model Estimation
```{r}
library(survival)
library(MASS)
```


```{r}
# set seed
set.seed(42)

# setting
# sample size
n <- 30000

rho <- 0.5
gamma <- 1.5

beta1 <- log(2)
beta2 <- log(2)
beta3 <- -1
```


#### i). Generate $Z_1$, $Z_2$, $W_1$, and $W_2$.
```{r}
# Covariates Z1, Z2, W1, W2 

# Z1, Z2 생성
mean_vector <- c(0, 0)
cov_matrix <- matrix(c(1, 0.75, 
                       0.75, 1), nrow = 2)

z_values <- mvrnorm(n = n, mu = mean_vector, Sigma = cov_matrix)

Z1 <- z_values[, 1]
Z2 <- z_values[, 2]

# W1, W2 생성
W1 <- rnorm(n, mean = 0, sd = 1)
W2 <- rbinom(n, size = 1, prob = 0.5)
```

#### ii). Generate u ~ uniform(0, 1)
```{r}
u <- runif(n, min = 0, max = 1)
```

#### iii). Since F(t) ~ uniform(0,1), u = 1-S(t)
- 이 식을 이용해 t에 대해 푸는 작업은 iv. 스텝에서 수행.
```{r}

```

#### iv). t를 구하는 식을 통해 T 생성
```{r}
lambda_t <- exp(beta1 * Z1 + beta2 * W1 + beta3 * W2)
T <- (-log(1 - u) / (lambda_t * rho))^(1 / gamma)
```

#### v). T 생성 반복
- 이미 이전 스텝에서 30,000번 반복 수행됨.
```{r}

```

#### Result).
```{r}
# 데이터 프레임 생성
data <- data.frame(T = T, Z1 = Z1, W1 = W1, W2 = W2)

# Cox Cox Proportional Hazards Model 적합
cox_model <- coxph(Surv(T) ~ Z1 + W1 + W2, data = data)
summary(cox_model)
```

#### 결과 해석
1. `coef`
- $Z_1$: $\beta_1 = 0.696734$
- $W_1$: $\beta_2 = 0.684798$
- $W_2$: $\beta_3 = −0.992539$
- $Z_1$, $W_1$, $W_2$ 각각이 실제 값인 $\log(2), \log(2), -1$과 가깝게 추정된 것을 확인할 수 있다.

2. `exp(coef)`
- $Z_1$: $2.0072 — Z1$이 1 단위 증가할 때, 위험률이 약 2배 증가함을 의미
- $W_1$: $1.9834 — W1$이 1 단위 증가할 때, 위험률이 약 1.98배 증가함을 의미
- $W_1$: $0.3706 — W2$가 1 단위 증가할 때, 위험률이 약 63$ 감소함을 의미

3. `se(coef)`, `Pr(>|z|)`
- $Z_1$: 0.006672
- $W_1$: 0.006683
- $W_2$: 0.012556
- 각 회귀계수의 표준 오차도 매우 낮고, p-valueeh <2e-16으로 매우 작은 것을 확인할 수 있음
- $Z_1, W_1, W_2$에 대한 계수들이 통계적으로 유의미하다는 것을 의미

4. `Concordance` (일치도)
- 0.742
- 이는 모델이 데이터의 순서를 얼마나 잘 예측하는지를 나타냄
- 일반적으로 0.5는 우연 수준, 1은 완벽한 예측을 의미하므로, 0.742는 비교적 높은 예측 성능을 의미

### (b) Repeated Simulation for Model Estimation Accuracy
```{r}
# 반복 횟수 설정
num_simulations <- 100

# 추정값과 표준 오차를 저장할 행렬 생성
beta_estimates <- matrix(NA, nrow = num_simulations, ncol = 3)
se_estimates <- matrix(NA, nrow = num_simulations, ncol = 3)
```


```{r}
# 100번 반복하여 Cox 모델 적합 및 추정값과 표준 오차 저장
for (i in 1:num_simulations) {
  # Step ii: u ~ uniform(0, 1) 생성
  u <- runif(n, min = 0, max = 1)
  
  # Step iv: T 생성
  T <- (-log(1 - u) / (lambda_t * rho))^(1 / gamma)
  
  # 데이터 프레임 생성
  data <- data.frame(T = T, Z1 = Z1, W1 = W1, W2 = W2)
  
  # Cox 모델 적합
  cox_model <- coxph(Surv(T) ~ Z1 + W1 + W2, data = data)
  
  # 추정값과 표준 오차 저장
  beta_estimates[i, ] <- cox_model$coefficients
  se_estimates[i, ] <- sqrt(diag(vcov(cox_model)))
}

# 각 추정값의 표본 평균과 표본 표준편차 계산
beta_means <- colMeans(beta_estimates)
se_means <- colMeans(se_estimates)
beta_sds <- apply(beta_estimates, 2, sd)
se_sds <- apply(se_estimates, 2, sd)

# 결과 출력
cat("100회 반복에 대한 추정 결과:\n")
cat("베타 추정값의 표본 평균: ", beta_means, "\n")
cat("베타 추정값의 표본 표준편차: ", beta_sds, "\n")
cat("표준 오차의 표본 평균: ", se_means, "\n")
cat("표준 오차의 표본 표준편차: ", se_sds, "\n")
```

#### 결과 해석
1. 베타 추정값의 표본 평균
- $\beta_1$의 추정값 표본 평균: 0.6931639 $\approx \log(2)$
- $\beta_2$의 추정값 표본 평균: 0.6933708 $\approx \log(2)$
- $\beta_3$의 추정값 표본 평균: −1.001591 $\approx -1$
- 모든 계수가 실제값에 근접하므로, 모델이 잘 추정되고 있음을 확인

2. 베타 추정값의 표본 표준편차
- $\beta_1$의 추정값 표본 표준편차: 0.007137426 
- $\beta_2$의 추정값 표본 표준편차: 0.00634256 
- $\beta_3$의 추정값 표본 표준편차: 0.01251695 
- 표본 표준편차가 크지 않으므로, 추정값의 변동성이 크지 않음을 의미

3. 표준 오차의 표본 평균, 표본 표준편차
- 각 추정값의 **표준 오차의 평균**을 추정된 값의 표준편차와 비교했을 때 비슷한 수준을 보임. 
- 따라서, 표준 오차 추정이 정확함을 나타내며, 일관되게 추정되고 있음을 알 수 있음.
- 또한 **표준 오차의 표본 표준편차** 값들이 작다는 것은 표준 오차 추정값이 반복마다 크게 변하지 않고 안정적이라는 것을 의미


### (c) Determining Censoring Rates for Exponential Distributions
```{r}
# Set a large sample size for generating T and C
n_large <- 1e6

# Generate Z1, W1, W2 for the large sample
set.seed(42)
z_values <- mvrnorm(n = n_large, mu = c(0, 0), Sigma = matrix(c(1, 0.75, 0.75, 1), 2))
Z1_large <- z_values[, 1]
W1_large <- rnorm(n_large, 0, 1)
W2_large <- rbinom(n_large, 1, 0.5)

# Generate large T sample
lambda_t_large <- exp(beta1 * Z1_large + beta2 * W1_large + beta3 * W2_large)
u_large <- runif(n_large, 0, 1)
T_large <- (-log(1 - u_large) / (lambda_t_large * rho))^(1 / gamma)

# Function to find lambda_C for a desired censoring rate with a large sample
find_lambda_c_large <- function(censor_rate_target, T_large, max_iter = 100, tol = 1e-3) {
  lambda_c <- 1  # Initial guess
  for (i in 1:max_iter) {
    C_large <- rexp(n_large, rate = lambda_c)  # Generate large C sample
    censor_rate <- mean(C_large < T_large)
    if (abs(censor_rate - censor_rate_target) < tol) break
    lambda_c <- lambda_c * (censor_rate_target / censor_rate)
  }
  return(lambda_c)
}

# Desired censoring rates
censoring_rates <- c(0.10, 0.30, 0.90, 0.95, 0.99)

# Find lambda_C for each censoring rate using the large sample
lambda_c_values_large <- sapply(censoring_rates, find_lambda_c_large, T_large = T_large)

# Output the results for large sample
censoring_results_large <- data.frame(CensoringRate = censoring_rates, LambdaC = lambda_c_values_large)
print(censoring_results_large)
```

### 결과 테이블 구조:
1. **CensoringRate (검열 비율):** \(C < T\) (검열된 경우)로 관찰되는 데이터의 비율. 
  - e.g., Censoring rate이 0.10이면 전체 데이터의 10%가 censoring.
2. **LambdaC:** Censoring rate을 달성하기 위해 \(C \sim \text{Exp}(\lambda_C)\)에서 사용된 \(\lambda_C\) 값. \(\lambda_C\)는 censoring time이 얼마나 빠르게 감소하는지를 결정

---

### 해석:
- 검열 비율이 낮을수록 (\(10\%\)) \(\lambda_C\) 값이 작다 (0.04364328). 이는 censoring time이 길어져 대부분의 \(T\) 값이 관측된다는 의미
- 검열 비율이 높을수록 (\(99\%\)) \(\lambda_C\) 값이 크다 (14.09668373). 이는 censoring time이 짧아 \(T\) 값이 거의 관측되지 않고 censoring되는 경우가 대부분이라는 의미
- 각 \(\lambda_C\) 값은 시뮬레이션에서 특정 censoring rate을 달성하기 위해 \(C \sim \text{Exp}(\lambda_C)\)로 설정된 값을 나타냄.

---

### 활용:
이 결과는 Part 1 (d)에서 \(C\)를 생성할 때 사용
각 Censoring rate에 대해 해당 \(\lambda_C\) 값을 사용하여 데이터를 생성하면 주어진 censoring rate에 맞는 시뮬레이션 데이터를 만들 수 있다.

추가적으로, 높은 censoring rate에서 추정량의 정확도가 어떻게 변하는지 분석하면서 모델 성능을 평가할 수 있다. 이는 censoring rate이 높아질수록 정보 손실이 증가하기 때문.

### (d) Model Performance under Different Censoring Rates - n=30,000
```{r}
# Simulation parameters
n <- 30000  # Sample size for all variables
num_simulations <- 100  # Number of simulations
censoring_rates <- c(0.10, 0.30, 0.90, 0.95, 0.99)  # Desired censoring rates

# Function to simulate and fit Cox models with censoring
simulate_with_censoring_fixed <- function(censor_rate_target, num_simulations, n, T, Z1, Z2, W1, W2, lambda_c) {
  # Initialize matrices to store beta estimates and standard errors
  beta_estimates <- matrix(NA, nrow = num_simulations, ncol = 4)  # Z1, Z2, W1, W2
  se_estimates <- matrix(NA, nrow = num_simulations, ncol = 4)
  
  for (i in 1:num_simulations) {
    # Step 1: Generate censoring times C
    C <- rexp(n, rate = lambda_c)
    
    # Step 2: Calculate observed times X and censoring indicator Delta
    X <- pmin(T, C)
    Delta <- as.numeric(T <= C)
    
    # Step 3: Create data frame with observed data
    data <- data.frame(X = X, Delta = Delta, Z1 = Z1, Z2 = Z2, W1 = W1, W2 = W2)
    
    # Step 4: Fit Cox proportional hazards model
    cox_model <- coxph(Surv(X, Delta) ~ Z1 + Z2 + W1 + W2, data = data)
    
    # Step 5: Save beta estimates and standard errors
    beta_estimates[i, ] <- cox_model$coefficients
    se_estimates[i, ] <- sqrt(diag(vcov(cox_model)))
  }
  
  # Step 6: Calculate mean and standard deviation for estimates and standard errors
  beta_means <- colMeans(beta_estimates)
  beta_sds <- apply(beta_estimates, 2, sd)
  se_means <- colMeans(se_estimates)
  se_sds <- apply(se_estimates, 2, sd)
  
  # Return results
  return(list(
    beta_means = beta_means,
    beta_sds = beta_sds,
    se_means = se_means,
    se_sds = se_sds
  ))
}


# From (c)
lambda_c_values <- censoring_results_large$'LambdaC'
censoring_rates <- c(0.10, 0.30, 0.90, 0.95, 0.99)

# Iterate over censoring rates and simulate
results <- list()
for (i in seq_along(censoring_rates)) {
  rate <- censoring_rates[i]
  lambda_c <- lambda_c_values[i]  # Use (c)'s precomputed lambda_c

  results[[as.character(rate)]] <- simulate_with_censoring_fixed(
    censor_rate_target = rate,
    num_simulations = num_simulations,
    n = n,
    T = T,
    Z1 = Z1,
    Z2 = Z2,
    W1 = W1,
    W2 = W2,
    lambda_c = lambda_c
  )
}

# Print results
for (rate in censoring_rates) {
  cat("Results for Censoring Rate:", rate, "\n")
  print(results[[as.character(rate)]])
  cat("\n")
}

```

### (d) Model Performance under Different Censoring Rates - n=10,000,000
```{r}
# Generate large Z1, Z2, W1, W2
# set.seed(42)
# z_values_large <- mvrnorm(n = n_large, mu = c(0, 0), Sigma = matrix(c(1, 0.75, 0.75, 1), 2))
# Z1_large <- z_values_large[, 1]
# Z2_large <- z_values_large[, 2]
# W1_large <- rnorm(n_large, mean = 0, sd = 1)
# W2_large <- rbinom(n_large, size = 1, prob = 0.5)
# 
# # Function to perform simulations with censoring
# simulate_with_censoring_fixed <- function(censor_rate_target, num_simulations, n_large, T_large, Z1_large, Z2_large, W1_large, W2_large, lambda_c) {
#   # Initialize matrices to store estimates and standard errors
#   beta_estimates <- matrix(NA, nrow = num_simulations, ncol = 4)  # Z1, Z2, W1, W2
#   se_estimates <- matrix(NA, nrow = num_simulations, ncol = 4)
#   
#   for (i in 1:num_simulations) {
#     # Step 1: Generate censoring times C
#     C_large <- rexp(n_large, rate = lambda_c)
#     
#     # Step 2: Calculate observed times X and censoring indicator Delta
#     X_large <- pmin(T_large, C_large)
#     Delta_large <- as.numeric(T_large <= C_large)
#     
#     # Step 3: Create data frame with observed data
#     data_large <- data.frame(X = X_large, Delta = Delta_large, Z1 = Z1_large, Z2 = Z2_large, W1 = W1_large, W2 = W2_large)
#     
#     # Step 4: Fit Cox model using observed data
#     cox_model <- coxph(Surv(X, Delta) ~ Z1 + Z2 + W1 + W2, data = data_large)
#     
#     # Step 5: Save estimates and standard errors
#     beta_estimates[i, ] <- cox_model$coefficients
#     se_estimates[i, ] <- sqrt(diag(vcov(cox_model)))
#   }
#   
#   # Step 6: Calculate means and standard deviations
#   beta_means <- colMeans(beta_estimates)
#   beta_sds <- apply(beta_estimates, 2, sd)
#   se_means <- colMeans(se_estimates)
#   se_sds <- apply(se_estimates, 2, sd)
#   
#   # Return results as a list
#   return(list(
#     beta_means = beta_means,
#     beta_sds = beta_sds,
#     se_means = se_means,
#     se_sds = se_sds
#   ))
# }
# 
# # Iterate over censoring rates and simulate
# results_fixed <- list()
# for (rate in censoring_rates) {
#   lambda_c <- lambda_c_values_large[which(censoring_rates == rate)]
#   
#   results_fixed[[as.character(rate)]] <- simulate_with_censoring_fixed(
#     censor_rate_target = rate,
#     num_simulations = num_simulations,
#     n_large = n_large,
#     T_large = T_large,
#     Z1_large = Z1_large,
#     Z2_large = Z2_large,
#     W1_large = W1_large,
#     W2_large = W2_large,
#     lambda_c = lambda_c
#   )
# }
# 
# # Print results for each censoring rate
# for (rate in censoring_rates) {
#   cat("Results for Censoring Rate:", rate, "\n")
#   print(results_fixed[[as.character(rate)]])
#   cat("\n")
# }
```

## Part 2.

### (a): Case-cohort design:
```{r}
# (a) Case-Cohort Sampling and Cox Model Fitting
case_cohort_sampling <- function(data, subcohort_size = 100) {
  # Step 1: Identify failures (Delta == 1)
  failures <- data[data$Delta == 1, ]
  non_failures <- data[data$Delta == 0, ]
  
  # Step 2: Sample the subcohort
  sampled_non_failures <- non_failures[sample(1:nrow(non_failures), subcohort_size, replace = FALSE), ]
  
  # Combine all failures and sampled non-failures
  subcohort <- rbind(failures, sampled_non_failures)
  
  # Calculate weights
  n_c <- nrow(non_failures)  # Total non-failures in the full cohort
  n_c_tilde <- nrow(sampled_non_failures)  # Sampled non-failures in subcohort
  subcohort$weights <- ifelse(subcohort$Delta == 1, 1, n_c / n_c_tilde)  # Assign weights
  
  # Fit Cox model with weights
  cox_model <- coxph(Surv(X, Delta) ~ Z1 + W1 + W2, data = subcohort, weights = weights)
  
  # Return results
  return(list(
    cox_model = cox_model,
    subcohort_size = nrow(subcohort),
    failures_count = nrow(failures)
  ))
}

# (a) Simulation for 500 Datasets
simulate_case_cohort <- function(datasets, subcohort_size = 100) {
  beta_estimates <- matrix(NA, nrow = length(datasets), ncol = 3)  # For β1, β2, β3
  subcohort_sizes <- numeric(length(datasets))
  failures_counts <- numeric(length(datasets))
  
  for (i in 1:length(datasets)) {
    data <- datasets[[i]]
    result <- case_cohort_sampling(data, subcohort_size = subcohort_size)
    
    # Save results
    beta_estimates[i, ] <- result$cox_model$coefficients
    subcohort_sizes[i] <- result$subcohort_size
    failures_counts[i] <- result$failures_count
  }
  
  # Calculate averages and standard deviations
  avg_failures <- mean(failures_counts)
  avg_subcohort_size <- mean(subcohort_sizes)
  beta_means <- colMeans(beta_estimates, na.rm = TRUE)
  beta_sds <- apply(beta_estimates, 2, sd, na.rm = TRUE)
  
  return(list(
    beta_means = beta_means,
    beta_sds = beta_sds,
    avg_failures = avg_failures,
    avg_subcohort_size = avg_subcohort_size
  ))
}

# 데이터 생성 (이미 Part 1에서 생성한 데이터 재활용)
datasets <- list()
for (i in 1:500) {
  C <- rexp(n, rate = 0.04364328)  # Using censoring rate 99% (from Part (c))
  X <- pmin(T, C)
  Delta <- as.numeric(T <= C)
  datasets[[i]] <- data.frame(X = X, Delta = Delta, Z1 = Z1, Z2 = Z2, W1 = W1, W2 = W2)
}

# (a) Case-cohort design with subcohort size 100
results_100 <- simulate_case_cohort(datasets, subcohort_size = 100)

# (a) Case-cohort design with subcohort size 300
results_300 <- simulate_case_cohort(datasets, subcohort_size = 300)

# Results for subcohort size 100
cat("Results for subcohort size 100:\n")
cat("Average failures:", results_100$avg_failures, "\n")
cat("Average subcohort size:", results_100$avg_subcohort_size, "\n")
cat("Beta means:", results_100$beta_means, "\n")
cat("Beta standard deviations:", results_100$beta_sds, "\n\n")

# Results for subcohort size 300
cat("Results for subcohort size 300:\n")
cat("Average failures:", results_300$avg_failures, "\n")
cat("Average subcohort size:", results_300$avg_subcohort_size, "\n")
cat("Beta means:", results_300$beta_means, "\n")
cat("Beta standard deviations:", results_300$beta_sds, "\n")
```



### (b): Nested case-cohort design:
```{r}
library(parallel)
library(survival)

nested_case_control_sampling <- function(data, control_size = 1) {
  failures <- data[data$Delta == 1, ]
  non_failures <- data[data$Delta == 0, ]
  
  nested_data <- failures
  
  # Pre-sort non_failures by failure time to reduce filtering overhead
  non_failures <- non_failures[order(non_failures$X), ]
  
  for (i in 1:nrow(failures)) {
    failure_time <- failures$X[i]
    
    # Use binary search for efficient at-risk filtering
    at_risk_controls <- non_failures[non_failures$X > failure_time, ]
    if (nrow(at_risk_controls) >= control_size) {
      sampled_controls <- at_risk_controls[sample(1:nrow(at_risk_controls), control_size, replace = FALSE), ]
      nested_data <- rbind(nested_data, sampled_controls)
    }
  }
  
  cox_model <- coxph(Surv(X, Delta) ~ Z1 + W1 + W2, data = nested_data)
  
  return(list(
    cox_model = cox_model,
    nested_size = nrow(nested_data),
    failures_count = nrow(failures)
  ))
}

simulate_nested_case_control_parallel <- function(datasets, control_size = 1, cores = 4) {
  cl <- makeCluster(cores)
  
  # Load required library on each node
  clusterEvalQ(cl, library(survival))
  
  # Export all necessary variables and functions to the cluster
  clusterExport(cl, varlist = c("nested_case_control_sampling", "control_size"), envir = environment())
  
  results <- parLapply(cl, datasets, function(data) {
    nested_case_control_sampling(data, control_size = control_size)
  })
  
  stopCluster(cl)
  
  beta_estimates <- do.call(rbind, lapply(results, function(res) res$cox_model$coefficients))
  nested_sizes <- sapply(results, function(res) res$nested_size)
  failures_counts <- sapply(results, function(res) res$failures_count)
  
  avg_failures <- mean(failures_counts)
  avg_nested_size <- mean(nested_sizes)
  beta_means <- colMeans(beta_estimates, na.rm = TRUE)
  beta_sds <- apply(beta_estimates, 2, sd, na.rm = TRUE)
  
  return(list(
    beta_means = beta_means,
    beta_sds = beta_sds,
    avg_failures = avg_failures,
    avg_nested_size = avg_nested_size
  ))
}

# 병렬 처리로 실행
results_control_1 <- simulate_nested_case_control_parallel(datasets, control_size = 1, cores = 4)
results_control_5 <- simulate_nested_case_control_parallel(datasets, control_size = 5, cores = 4)

# 결과 출력
cat("Results for control size 1:\n")
cat("Average failures:", results_control_1$avg_failures, "\n")
cat("Average nested size:", results_control_1$avg_nested_size, "\n")
cat("Beta means:", results_control_1$beta_means, "\n")
cat("Beta standard deviations:", results_control_1$beta_sds, "\n\n")

cat("Results for control size 5:\n")
cat("Average failures:", results_control_5$avg_failures, "\n")
cat("Average nested size:", results_control_5$avg_nested_size, "\n")
cat("Beta means:", results_control_5$beta_means, "\n")
cat("Beta standard deviations:", results_control_5$beta_sds, "\n")

```

## Part 3: End-Point Sampling
```{r}
datasets <- list()
for (i in 1:500) {
  C <- rexp(n, rate = 0.04364328)  # Using censoring rate 99% (from Part (c))
  X <- pmin(T, C)
  Delta <- as.numeric(T <= C)
  datasets[[i]] <- data.frame(X = X, Delta = Delta, Z1 = Z1, Z2 = Z2, W1 = W1, W2 = W2)
}
```

```{r}
# End-point Sampling Function
end_point_sampling <- function(data, control_sample_size = 100) {
  # Step 1: Identify cases and controls
  cases <- data[data$Delta == 1, ]  # Case 데이터
  controls <- data[data$Delta == 0, ]  # Control 데이터
  
  # Step 2: Sort controls by censoring time (X) in descending order
  controls_sorted <- controls[order(controls$X, decreasing = TRUE), ]
  
  # Step 3: Select top control_sample_size controls
  sampled_controls <- head(controls_sorted, control_sample_size)
  
  # Combine cases and sampled controls
  sampled_data <- rbind(cases, sampled_controls)
  
  # Return the sampled data
  return(sampled_data)
}

# Function to fit Cox model with end-point sampling
simulate_end_point_sampling <- function(datasets, control_sample_size = 100) {
  beta_estimates <- matrix(NA, nrow = length(datasets), ncol = 3)  # For β1, β2, β3
  case_counts <- numeric(length(datasets))
  control_counts <- numeric(length(datasets))
  
  for (i in 1:length(datasets)) {
    data <- datasets[[i]]
    
    # Perform End-point Sampling
    sampled_data <- end_point_sampling(data, control_sample_size = control_sample_size)
    
    # Fit Cox model using sampled data
    cox_model <- coxph(Surv(X, Delta) ~ Z1 + W1 + W2, data = sampled_data)
    
    # Save beta estimates
    beta_estimates[i, ] <- cox_model$coefficients
    
    # Save case and control counts
    case_counts[i] <- nrow(sampled_data[sampled_data$Delta == 1, ])
    control_counts[i] <- nrow(sampled_data[sampled_data$Delta == 0, ])
  }
  
  # Calculate summary statistics
  beta_means <- colMeans(beta_estimates, na.rm = TRUE)
  beta_sds <- apply(beta_estimates, 2, sd, na.rm = TRUE)
  avg_case_count <- mean(case_counts)
  avg_control_count <- mean(control_counts)
  
  return(list(
    beta_means = beta_means,
    beta_sds = beta_sds,
    avg_case_count = avg_case_count,
    avg_control_count = avg_control_count
  ))
}

# Example usage with generated datasets
results_end_point_100 <- simulate_end_point_sampling(datasets, control_sample_size = 100)
results_end_point_300 <- simulate_end_point_sampling(datasets, control_sample_size = 300)
results_end_point_500 <- simulate_end_point_sampling(datasets, control_sample_size = 500)
results_end_point_1000 <- simulate_end_point_sampling(datasets, control_sample_size = 1000)

# Print results for control sample size 100
cat("Results for End-point Sampling (Control Sample Size = 100):\n")
cat("Average case count:", results_end_point_100$avg_case_count, "\n")
cat("Average control count:", results_end_point_100$avg_control_count, "\n")
cat("Beta means:", results_end_point_100$beta_means, "\n")
cat("Beta standard deviations:", results_end_point_100$beta_sds, "\n\n")

# Print results for control sample size 300
cat("Results for End-point Sampling (Control Sample Size = 300):\n")
cat("Average case count:", results_end_point_300$avg_case_count, "\n")
cat("Average control count:", results_end_point_300$avg_control_count, "\n")
cat("Beta means:", results_end_point_300$beta_means, "\n")
cat("Beta standard deviations:", results_end_point_300$beta_sds, "\n")

# Print results for control sample size 500
cat("Results for End-point Sampling (Control Sample Size = 500):\n")
cat("Average case count:", results_end_point_500$avg_case_count, "\n")
cat("Average control count:", results_end_point_500$avg_control_count, "\n")
cat("Beta means:", results_end_point_500$beta_means, "\n")
cat("Beta standard deviations:", results_end_point_500$beta_sds, "\n")

# Print results for control sample size 1000
cat("Results for End-point Sampling (Control Sample Size = 1000):\n")
cat("Average case count:", results_end_point_1000$avg_case_count, "\n")
cat("Average control count:", results_end_point_1000$avg_control_count, "\n")
cat("Beta means:", results_end_point_1000$beta_means, "\n")
cat("Beta standard deviations:", results_end_point_1000$beta_sds, "\n")
```

#### 3000번
```{r}
datasets <- list()
for (i in 1:3000) {
  C <- rexp(n, rate = 0.04364328)  # Using censoring rate 99% (from Part (c))
  X <- pmin(T, C)
  Delta <- as.numeric(T <= C)
  datasets[[i]] <- data.frame(X = X, Delta = Delta, Z1 = Z1, Z2 = Z2, W1 = W1, W2 = W2)
}

# Example usage with generated datasets
results_end_point_100 <- simulate_end_point_sampling(datasets, control_sample_size = 100)
results_end_point_300 <- simulate_end_point_sampling(datasets, control_sample_size = 300)

# Print results for control sample size 100
cat("Results for End-point Sampling (Control Sample Size = 100):\n")
cat("Average case count:", results_end_point_100$avg_case_count, "\n")
cat("Average control count:", results_end_point_100$avg_control_count, "\n")
cat("Beta means:", results_end_point_100$beta_means, "\n")
cat("Beta standard deviations:", results_end_point_100$beta_sds, "\n\n")

# Print results for control sample size 300
cat("Results for End-point Sampling (Control Sample Size = 300):\n")
cat("Average case count:", results_end_point_300$avg_case_count, "\n")
cat("Average control count:", results_end_point_300$avg_control_count, "\n")
cat("Beta means:", results_end_point_300$beta_means, "\n")
cat("Beta standard deviations:", results_end_point_300$beta_sds, "\n")
```









