---
title: "STAT9050_Mid_Code_HyubKim"
author: "김협"
output: html_document
---

# STAT9050-01. 응용통계학 특수문제 I

## Part 1. 

### (a) Survival Times Simulation and Cox Proportional Hazards Model Estimation
```{r}
library(survival)
library(MASS)
```


```{r}
# set seed
set.seed(42)

# setting
# sample size
n <- 30000

rho <- 0.5
gamma <- 1.5

beta1 <- log(2)
beta2 <- log(2)
beta3 <- -1
```


#### i). Generate $Z_1$, $Z_2$, $W_1$, and $W_2$.
```{r}
# Covariates Z1, Z2, W1, W2 

# Z1, Z2 생성
mean_vector <- c(0, 0)
cov_matrix <- matrix(c(1, 0.75, 
                       0.75, 1), nrow = 2)

z_values <- mvrnorm(n = n, mu = mean_vector, Sigma = cov_matrix)

Z1 <- z_values[, 1]
Z2 <- z_values[, 2]

# W1, W2 생성
W1 <- rnorm(n, mean = 0, sd = 1)
W2 <- rbinom(n, size = 1, prob = 0.5)
```

#### ii). Generate u ~ uniform(0, 1)
```{r}
u <- runif(n, min = 0, max = 1)
```

#### iii). Since F(t) ~ uniform(0,1), u = 1-S(t)
- 이 식을 이용해 t에 대해 푸는 작업은 iv. 스텝에서 수행.
```{r}

```

#### iv). t를 구하는 식을 통해 T 생성
```{r}
lambda_t <- exp(beta1 * Z1 + beta2 * W1 + beta3 * W2)
T <- (-log(1 - u) / (lambda_t * rho))^(1 / gamma)
```

#### v). T 생성 반복
- 이미 이전 스텝에서 30,000번 반복 수행됨.
```{r}

```

#### Result).
```{r}
# 데이터 프레임 생성
data <- data.frame(T = T, Z1 = Z1, W1 = W1, W2 = W2)

# Cox Cox Proportional Hazards Model 적합
cox_model <- coxph(Surv(T) ~ Z1 + W1 + W2, data = data)
summary(cox_model)
```

#### 결과 해석
1. `coef`
- $Z_1$: $\beta_1 = 0.696734$
- $W_1$: $\beta_2 = 0.684798$
- $W_2$: $\beta_3 = −0.992539$
- $Z_1$, $W_1$, $W_2$ 각각이 실제 값인 $\log(2), \log(2), -1$과 가깝게 추정된 것을 확인할 수 있다.

2. `exp(coef)`
- $Z_1$: $2.0072 — Z1$이 1 단위 증가할 때, 위험률이 약 2배 증가함을 의미
- $W_1$: $1.9834 — W1$이 1 단위 증가할 때, 위험률이 약 1.98배 증가함을 의미
- $W_1$: $0.3706 — W2$가 1 단위 증가할 때, 위험률이 약 63$ 감소함을 의미

3. `se(coef)`, `Pr(>|z|)`
- $Z_1$: 0.006672
- $W_1$: 0.006683
- $W_2$: 0.012556
- 각 회귀계수의 표준 오차도 매우 낮고, p-valueeh <2e-16으로 매우 작은 것을 확인할 수 있음
- $Z_1, W_1, W_2$에 대한 계수들이 통계적으로 유의미하다는 것을 의미

4. `Concordance` (일치도)
- 0.742
- 이는 모델이 데이터의 순서를 얼마나 잘 예측하는지를 나타냄
- 일반적으로 0.5는 우연 수준, 1은 완벽한 예측을 의미하므로, 0.742는 비교적 높은 예측 성능을 의미

### (b) Repeated Simulation for Model Estimation Accuracy
```{r}
# 반복 횟수 설정
num_simulations <- 100

# 추정값과 표준 오차를 저장할 행렬 생성
beta_estimates <- matrix(NA, nrow = num_simulations, ncol = 3)
se_estimates <- matrix(NA, nrow = num_simulations, ncol = 3)
```


```{r}
# 100번 반복하여 Cox 모델 적합 및 추정값과 표준 오차 저장
for (i in 1:num_simulations) {
  # Step ii: u ~ uniform(0, 1) 생성
  u <- runif(n, min = 0, max = 1)
  
  # Step iv: T 생성
  T <- (-log(1 - u) / (lambda_t * rho))^(1 / gamma)
  
  # 데이터 프레임 생성
  data <- data.frame(T = T, Z1 = Z1, W1 = W1, W2 = W2)
  
  # Cox 모델 적합
  cox_model <- coxph(Surv(T) ~ Z1 + W1 + W2, data = data)
  
  # 추정값과 표준 오차 저장
  beta_estimates[i, ] <- cox_model$coefficients
  se_estimates[i, ] <- sqrt(diag(vcov(cox_model)))
}

# 각 추정값의 표본 평균과 표본 표준편차 계산
beta_means <- colMeans(beta_estimates)
se_means <- colMeans(se_estimates)
beta_sds <- apply(beta_estimates, 2, sd)
se_sds <- apply(se_estimates, 2, sd)

# 결과 출력
cat("100회 반복에 대한 추정 결과:\n")
cat("베타 추정값의 표본 평균: ", beta_means, "\n")
cat("베타 추정값의 표본 표준편차: ", beta_sds, "\n")
cat("표준 오차의 표본 평균: ", se_means, "\n")
cat("표준 오차의 표본 표준편차: ", se_sds, "\n")
```

#### 결과 해석
1. 베타 추정값의 표본 평균
- $\beta_1$의 추정값 표본 평균: 0.6931639 $\approx \log(2)$
- $\beta_2$의 추정값 표본 평균: 0.6933708 $\approx \log(2)$
- $\beta_3$의 추정값 표본 평균: −1.001591 $\approx -1$
- 모든 계수가 실제값에 근접하므로, 모델이 잘 추정되고 있음을 확인

2. 베타 추정값의 표본 표준편차
- $\beta_1$의 추정값 표본 표준편차: 0.007137426 
- $\beta_2$의 추정값 표본 표준편차: 0.00634256 
- $\beta_3$의 추정값 표본 표준편차: 0.01251695 
- 표본 표준편차가 크지 않으므로, 추정값의 변동성이 크지 않음을 의미

3. 표준 오차의 표본 평균, 표본 표준편차
- 각 추정값의 **표준 오차의 평균**을 추정된 값의 표준편차와 비교했을 때 비슷한 수준을 보임. 
- 따라서, 표준 오차 추정이 정확함을 나타내며, 일관되게 추정되고 있음을 알 수 있음.
- 또한 **표준 오차의 표본 표준편차** 값들이 작다는 것은 표준 오차 추정값이 반복마다 크게 변하지 않고 안정적이라는 것을 의미




### (c) Determining Censoring Rates for Exponential Distributions
```{r}
# Set a large sample size for generating T and C
n_large <- 1e6

# Generate Z1, W1, W2 for the large sample
set.seed(42)
z_values <- mvrnorm(n = n_large, mu = c(0, 0), Sigma = matrix(c(1, 0.75, 0.75, 1), 2))
Z1_large <- z_values[, 1]
W1_large <- rnorm(n_large, 0, 1)
W2_large <- rbinom(n_large, 1, 0.5)

# Generate large T sample
lambda_t_large <- exp(beta1 * Z1_large + beta2 * W1_large + beta3 * W2_large)
u_large <- runif(n_large, 0, 1)
T_large <- (-log(1 - u_large) / (lambda_t_large * rho))^(1 / gamma)

# Function to find lambda_C for a desired censoring rate with a large sample
find_lambda_c_large <- function(censor_rate_target, T_large, max_iter = 100, tol = 1e-3) {
  lambda_c <- 1  # Initial guess
  for (i in 1:max_iter) {
    C_large <- rexp(n_large, rate = lambda_c)  # Generate large C sample
    censor_rate <- mean(C_large < T_large)
    if (abs(censor_rate - censor_rate_target) < tol) break
    lambda_c <- lambda_c * (censor_rate_target / censor_rate)
  }
  return(lambda_c)
}

# Desired censoring rates
censoring_rates <- c(0.10, 0.30, 0.90, 0.95, 0.99)

# Find lambda_C for each censoring rate using the large sample
lambda_c_values_large <- sapply(censoring_rates, find_lambda_c_large, T_large = T_large)

# Output the results for large sample
censoring_results_large <- data.frame(CensoringRate = censoring_rates, LambdaC = lambda_c_values_large)
print(censoring_results_large)
```

### 결과 테이블 구조:
1. **CensoringRate (검열 비율):** \(C < T\) (검열된 경우)로 관찰되는 데이터의 비율. 
  - e.g., Censoring rate이 0.10이면 전체 데이터의 10%가 censoring.
2. **LambdaC:** Censoring rate을 달성하기 위해 \(C \sim \text{Exp}(\lambda_C)\)에서 사용된 \(\lambda_C\) 값. \(\lambda_C\)는 censoring time이 얼마나 빠르게 감소하는지를 결정

---

### 해석:
- 검열 비율이 낮을수록 (\(10\%\)) \(\lambda_C\) 값이 작다 (0.04364328). 이는 censoring time이 길어져 대부분의 \(T\) 값이 관측된다는 의미
- 검열 비율이 높을수록 (\(99\%\)) \(\lambda_C\) 값이 크다 (14.09668373). 이는 censoring time이 짧아 \(T\) 값이 거의 관측되지 않고 censoring되는 경우가 대부분이라는 의미
- 각 \(\lambda_C\) 값은 시뮬레이션에서 특정 censoring rate을 달성하기 위해 \(C \sim \text{Exp}(\lambda_C)\)로 설정된 값을 나타냄.

---

### 활용:
이 결과는 Part 1 (d)에서 \(C\)를 생성할 때 사용
각 Censoring rate에 대해 해당 \(\lambda_C\) 값을 사용하여 데이터를 생성하면 주어진 censoring rate에 맞는 시뮬레이션 데이터를 만들 수 있다.

추가적으로, 높은 censoring rate에서 추정량의 정확도가 어떻게 변하는지 분석하면서 모델 성능을 평가할 수 있다. 이는 censoring rate이 높아질수록 정보 손실이 증가하기 때문.

### (d) Model Performance under Different Censoring Rates - n=30,000
```{r}
# Simulation parameters
n <- 30000  # Sample size for all variables
num_simulations <- 100  # Number of simulations
censoring_rates <- c(0.10, 0.30, 0.90, 0.95, 0.99)  # Desired censoring rates

# Function to simulate and fit Cox models with censoring
simulate_with_censoring_fixed <- function(censor_rate_target, num_simulations, n, T, Z1, Z2, W1, W2, lambda_c) {
  # Initialize matrices to store beta estimates and standard errors
  beta_estimates <- matrix(NA, nrow = num_simulations, ncol = 4)  # Z1, Z2, W1, W2
  se_estimates <- matrix(NA, nrow = num_simulations, ncol = 4)
  
  for (i in 1:num_simulations) {
    # Step 1: Generate censoring times C
    C <- rexp(n, rate = lambda_c)
    
    # Step 2: Calculate observed times X and censoring indicator Delta
    X <- pmin(T, C)
    Delta <- as.numeric(T <= C)
    
    # Step 3: Create data frame with observed data
    data <- data.frame(X = X, Delta = Delta, Z1 = Z1, Z2 = Z2, W1 = W1, W2 = W2)
    
    # Step 4: Fit Cox proportional hazards model
    cox_model <- coxph(Surv(X, Delta) ~ Z1 + Z2 + W1 + W2, data = data)
    
    # Step 5: Save beta estimates and standard errors
    beta_estimates[i, ] <- cox_model$coefficients
    se_estimates[i, ] <- sqrt(diag(vcov(cox_model)))
  }
  
  # Step 6: Calculate mean and standard deviation for estimates and standard errors
  beta_means <- colMeans(beta_estimates)
  beta_sds <- apply(beta_estimates, 2, sd)
  se_means <- colMeans(se_estimates)
  se_sds <- apply(se_estimates, 2, sd)
  
  # Return results
  return(list(
    beta_means = beta_means,
    beta_sds = beta_sds,
    se_means = se_means,
    se_sds = se_sds
  ))
}


# From (c)
lambda_c_values <- censoring_results_large$'LambdaC'
censoring_rates <- c(0.10, 0.30, 0.90, 0.95, 0.99)

# Iterate over censoring rates and simulate
results <- list()
for (i in seq_along(censoring_rates)) {
  rate <- censoring_rates[i]
  lambda_c <- lambda_c_values[i]  # Use (c)'s precomputed lambda_c

  results[[as.character(rate)]] <- simulate_with_censoring_fixed(
    censor_rate_target = rate,
    num_simulations = num_simulations,
    n = n,
    T = T,
    Z1 = Z1,
    Z2 = Z2,
    W1 = W1,
    W2 = W2,
    lambda_c = lambda_c
  )
}

# Print results
for (rate in censoring_rates) {
  cat("Results for Censoring Rate:", rate, "\n")
  print(results[[as.character(rate)]])
  cat("\n")
}

```

### (d) Model Performance under Different Censoring Rates - n=10,000,000
```{r}
# Generate large Z1, Z2, W1, W2
set.seed(42)
z_values_large <- mvrnorm(n = n_large, mu = c(0, 0), Sigma = matrix(c(1, 0.75, 0.75, 1), 2))
Z1_large <- z_values_large[, 1]
Z2_large <- z_values_large[, 2]
W1_large <- rnorm(n_large, mean = 0, sd = 1)
W2_large <- rbinom(n_large, size = 1, prob = 0.5)

# Function to perform simulations with censoring
simulate_with_censoring_fixed <- function(censor_rate_target, num_simulations, n_large, T_large, Z1_large, Z2_large, W1_large, W2_large, lambda_c) {
  # Initialize matrices to store estimates and standard errors
  beta_estimates <- matrix(NA, nrow = num_simulations, ncol = 4)  # Z1, Z2, W1, W2
  se_estimates <- matrix(NA, nrow = num_simulations, ncol = 4)
  
  for (i in 1:num_simulations) {
    # Step 1: Generate censoring times C
    C_large <- rexp(n_large, rate = lambda_c)
    
    # Step 2: Calculate observed times X and censoring indicator Delta
    X_large <- pmin(T_large, C_large)
    Delta_large <- as.numeric(T_large <= C_large)
    
    # Step 3: Create data frame with observed data
    data_large <- data.frame(X = X_large, Delta = Delta_large, Z1 = Z1_large, Z2 = Z2_large, W1 = W1_large, W2 = W2_large)
    
    # Step 4: Fit Cox model using observed data
    cox_model <- coxph(Surv(X, Delta) ~ Z1 + Z2 + W1 + W2, data = data_large)
    
    # Step 5: Save estimates and standard errors
    beta_estimates[i, ] <- cox_model$coefficients
    se_estimates[i, ] <- sqrt(diag(vcov(cox_model)))
  }
  
  # Step 6: Calculate means and standard deviations
  beta_means <- colMeans(beta_estimates)
  beta_sds <- apply(beta_estimates, 2, sd)
  se_means <- colMeans(se_estimates)
  se_sds <- apply(se_estimates, 2, sd)
  
  # Return results as a list
  return(list(
    beta_means = beta_means,
    beta_sds = beta_sds,
    se_means = se_means,
    se_sds = se_sds
  ))
}

# Iterate over censoring rates and simulate
results_fixed <- list()
for (rate in censoring_rates) {
  lambda_c <- lambda_c_values_large[which(censoring_rates == rate)]
  
  results_fixed[[as.character(rate)]] <- simulate_with_censoring_fixed(
    censor_rate_target = rate,
    num_simulations = num_simulations,
    n_large = n_large,
    T_large = T_large,
    Z1_large = Z1_large,
    Z2_large = Z2_large,
    W1_large = W1_large,
    W2_large = W2_large,
    lambda_c = lambda_c
  )
}

# Print results for each censoring rate
for (rate in censoring_rates) {
  cat("Results for Censoring Rate:", rate, "\n")
  print(results_fixed[[as.character(rate)]])
  cat("\n")
}

```

## Part 2.

### (a): Case-cohort design:
```{r}

```


### (b): Nested case-cohort design:
```{r}

```



