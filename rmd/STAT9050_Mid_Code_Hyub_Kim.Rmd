---
title: "STAT9050_Mid_Code_HyubKim"
author: "김협"
output: html_document
---

# STAT9050-01. 응용통계학 특수문제 I

## Part 1. 

### (a) Survival Times Simulation and Cox Proportional Hazards Model Estimation
```{r}
library(survival)
library(MASS)
```


```{r}
# set seed
set.seed(42)

# setting
# sample size
n <- 30000

rho <- 0.5
gamma <- 1.5

beta1 <- log(2)
beta2 <- log(2)
beta3 <- -1
```


#### i). Generate $Z_1$, $Z_2$, $W_1$, and $W_2$.
```{r}
# Covariates Z1, Z2, W1, W2 

# Z1, Z2 생성
mean_vector <- c(0, 0)
cov_matrix <- matrix(c(1, 0.75, 
                       0.75, 1), nrow = 2)

z_values <- mvrnorm(n = n, mu = mean_vector, Sigma = cov_matrix)

Z1 <- z_values[, 1]
Z2 <- z_values[, 2]

# W1, W2 생성
W1 <- rnorm(n, mean = 0, sd = 1)
W2 <- rbinom(n, size = 1, prob = 0.5)
```

#### ii). Generate u ~ uniform(0, 1)
```{r}
u <- runif(n, min = 0, max = 1)
```

#### iii). Since F(t) ~ uniform(0,1), u = 1-S(t)
- 이 식을 이용해 t에 대해 푸는 작업은 iv. 스텝에서 수행.
```{r}

```

#### iv). t를 구하는 식을 통해 T 생성
```{r}
lambda_t <- exp(beta1 * Z1 + beta2 * W1 + beta3 * W2)
T <- (-log(1 - u) / (lambda_t * rho))^(1 / gamma)
```

#### v). T 생성 반복
- 이미 이전 스텝에서 30,000번 반복 수행됨.
```{r}

```

#### Result).
```{r}
# 데이터 프레임 생성
data <- data.frame(T = T, Z1 = Z1, W1 = W1, W2 = W2)

# Cox Cox Proportional Hazards Model 적합
cox_model <- coxph(Surv(T) ~ Z1 + W1 + W2, data = data)
summary(cox_model)
```

#### 결과 해석
1. `coef`
- $Z_1$: $\beta_1 = 0.696734$
- $W_1$: $\beta_2 = 0.684798$
- $W_2$: $\beta_3 = −0.992539$
- $Z_1$, $W_1$, $W_2$ 각각이 실제 값인 $\log(2), \log(2), -1$과 가깝게 추정된 것을 확인할 수 있다.

2. `exp(coef)`
- $Z_1$: $2.0072 — Z1$이 1 단위 증가할 때, 위험률이 약 2배 증가함을 의미
- $W_1$: $1.9834 — W1$이 1 단위 증가할 때, 위험률이 약 1.98배 증가함을 의미
- $W_1$: $0.3706 — W2$가 1 단위 증가할 때, 위험률이 약 63$ 감소함을 의미

3. `se(coef)`, `Pr(>|z|)`
- $Z_1$: 0.006672
- $W_1$: 0.006683
- $W_2$: 0.012556
- 각 회귀계수의 표준 오차도 매우 낮고, p-valueeh <2e-16으로 매우 작은 것을 확인할 수 있음
- $Z_1, W_1, W_2$에 대한 계수들이 통계적으로 유의미하다는 것을 의미

4. `Concordance` (일치도)
- 0.742
- 이는 모델이 데이터의 순서를 얼마나 잘 예측하는지를 나타냄
- 일반적으로 0.5는 우연 수준, 1은 완벽한 예측을 의미하므로, 0.742는 비교적 높은 예측 성능을 의미

### (b) Repeated Simulation for Model Estimation Accuracy
```{r}
# 반복 횟수 설정
num_simulations <- 100

# 추정값과 표준 오차를 저장할 행렬 생성
beta_estimates <- matrix(NA, nrow = num_simulations, ncol = 3)
se_estimates <- matrix(NA, nrow = num_simulations, ncol = 3)
```


```{r}
# Simulation loop
for (i in 1:num_simulations) {
  # Generate u
  u <- runif(n, min = 0, max = 1)
  
  # Calculate lambda_t and generate T
  lambda_t <- exp(beta1 * Z1 + beta2 * W1 + beta3 * W2)
  T <- (-log(1 - u) / (lambda_t * rho))^(1 / gamma)
  
  # Create data frame
  data <- data.frame(T = T, Z1 = Z1, W1 = W1, W2 = W2)
  
  # Fit Cox model
  cox_model <- coxph(Surv(T) ~ Z1 + W1 + W2, data = data)
  
  # Store estimates and SEs
  beta_estimates[i, ] <- cox_model$coefficients
  se_estimates[i, ] <- sqrt(diag(vcov(cox_model)))
}

# Calculate means and standard deviations
beta_means <- colMeans(beta_estimates)
beta_sds <- apply(beta_estimates, 2, sd)
se_means <- colMeans(se_estimates)
se_sds <- apply(se_estimates, 2, sd)

# Present results as a data frame
results <- data.frame(
  Parameter = c("Beta1", "Beta2", "Beta3"),
  Mean_Estimate = beta_means,
  SD_Estimate = beta_sds,
  Mean_SE = se_means,
  SD_SE = se_sds
)

print("100회 반복 시뮬레이션 결과:")
print(results)
```

#### 결과 해석
1. 베타 추정값의 표본 평균
- $\beta_1$의 추정값 표본 평균: 0.6931428	 $\approx \log(2)$
- $\beta_2$의 추정값 표본 평균: 0.6940455 $\approx \log(2)$
- $\beta_3$의 추정값 표본 평균: −0.9991584 $\approx -1$
- 모든 계수가 실제값에 근접하므로, 모델이 잘 추정되고 있음을 확인

2. 베타 추정값의 표본 표준편차
- $\beta_1$의 추정값 표본 표준편차: 0.007425564 
- $\beta_2$의 추정값 표본 표준편차: 0.006778623 
- $\beta_3$의 추정값 표본 표준편차: 0.012908413 
- 표본 표준편차가 크지 않으므로, 추정값의 변동성이 크지 않음을 의미

3. 표준 오차의 표본 평균, 표본 표준편차
- 각 추정값의 **표준 오차의 평균**을 추정된 값의 표준편차와 비교했을 때 비슷한 수준을 보임. 
- 따라서, 표준 오차 추정이 정확함을 나타내며, 일관되게 추정되고 있음을 알 수 있음.
- 또한 **표준 오차의 표본 표준편차** 값들이 작다는 것은 표준 오차 추정값이 반복마다 크게 변하지 않고 안정적이라는 것을 의미


### (c) Determining Censoring Rates for Exponential Distributions
```{r}
# Set a large sample size for generating T and C
n_large <- 1e6

# Generate Z1, W1, W2 for the large sample
set.seed(42)
z_values <- mvrnorm(n = n_large, mu = c(0, 0), Sigma = matrix(c(1, 0.75, 0.75, 1), 2))
Z1_large <- z_values[, 1]
W1_large <- rnorm(n_large, 0, 1)
W2_large <- rbinom(n_large, 1, 0.5)

# Generate large T sample
lambda_t_large <- exp(beta1 * Z1_large + beta2 * W1_large + beta3 * W2_large)
u_large <- runif(n_large, 0, 1)
T_large <- (-log(1 - u_large) / (lambda_t_large * rho))^(1 / gamma)

# Function to find lambda_C for a desired censoring rate with a large sample
find_lambda_c_large <- function(censor_rate_target, T_large, max_iter = 200, tol = 1e-3) {
  lambda_c <- 1  # Initial guess
  iter_count <- 0  # To track iterations
  for (i in 1:max_iter) {
    C_large <- rexp(length(T_large), rate = lambda_c)  # Generate large C sample
    censor_rate <- mean(C_large < T_large)
    iter_count <- iter_count + 1
    if (abs(censor_rate - censor_rate_target) < tol) break
    lambda_c <- lambda_c * (censor_rate_target / censor_rate)
  }
  return(list(lambda_c = lambda_c, iterations = iter_count))
}

# Desired censoring rates
censoring_rates <- c(0.10, 0.30, 0.90, 0.95, 0.99)

# Find lambda_C for each censoring rate using the large sample
results <- lapply(censoring_rates, function(rate) {
  find_lambda_c_large(censor_rate_target = rate, T_large = T_large)
})

# Extract lambda_C and iteration counts
lambda_c_values_large <- sapply(results, function(res) res$lambda_c)
iteration_counts <- sapply(results, function(res) res$iterations)

# Validate calculated lambda_C values
validate_censoring_rate <- function(lambda_c, T_large) {
  C_large <- rexp(length(T_large), rate = lambda_c)
  return(mean(C_large < T_large))
}

# Validate censoring rates for each lambda_C
validated_rates <- sapply(lambda_c_values_large, validate_censoring_rate, T_large = T_large)

# Create a data frame to summarize results
censoring_results_large <- data.frame(
  CensoringRate = censoring_rates,
  LambdaC = lambda_c_values_large,
  Iterations = iteration_counts,
  ValidatedCensoringRate = validated_rates,
  Difference = censoring_rates - validated_rates
)

# Print the results
print(censoring_results_large)
```


### 결과 테이블 구조:
1. **CensoringRate (검열 비율):** \(C < T\) (검열된 경우)로 관찰되는 데이터의 비율. 
  - e.g., Censoring rate이 0.10이면 전체 데이터의 10%가 censoring.
2. **LambdaC:** Censoring rate을 달성하기 위해 \(C \sim \text{Exp}(\lambda_C)\)에서 사용된 \(\lambda_C\) 값. \(\lambda_C\)는 censoring time이 얼마나 빠르게 감소하는지를 결정
3. **Iterations:** 수렴하기까지의 실제 반복수 (max_itertion을 100에서 200으로 수정하여 CensoringRate이 0.99인 경우에 대해 더 정확한 값을 찾음)

---

### 해석:
- 검열 비율이 낮을수록 (\(10\%\)) \(\lambda_C\) 값이 작다 (0.04348288). 이는 censoring time이 길어져 대부분의 \(T\) 값이 관측된다는 의미
- 검열 비율이 높을수록 (\(99\%\)) \(\lambda_C\) 값이 크다 (15.92681178). 이는 censoring time이 짧아 \(T\) 값이 거의 관측되지 않고 censoring되는 경우가 대부분이라는 의미
- 각 \(\lambda_C\) 값은 시뮬레이션에서 특정 censoring rate을 달성하기 위해 \(C \sim \text{Exp}(\lambda_C)\)로 설정된 값을 나타냄.

---

### 활용:
이 결과는 Part 1 (d)에서 \(C\)를 생성할 때 사용
각 Censoring rate에 대해 해당 \(\lambda_C\) 값을 사용하여 데이터를 생성하면 주어진 censoring rate에 맞는 시뮬레이션 데이터를 만들 수 있다.

추가적으로, 높은 censoring rate에서 추정량의 정확도가 어떻게 변하는지 분석하면서 모델 성능을 평가할 수 있다. 이는 censoring rate이 높아질수록 정보 손실이 증가하기 때문.


### (d) Model Performance under Different Censoring Rates - n=30,000
```{r}
library(parallel)

# Function to simulate and fit Cox models with censoring
simulate_with_censoring_fixed <- function(censor_rate_target, num_simulations, n, T, Z1, Z2, W1, W2, lambda_c) {
  beta_estimates <- matrix(NA, nrow = num_simulations, ncol = 4)  # Z1, Z2, W1, W2
  se_estimates <- matrix(NA, nrow = num_simulations, ncol = 4)
  
  for (i in 1:num_simulations) {
    C <- rexp(n, rate = lambda_c)
    X <- pmin(T, C)
    Delta <- as.numeric(T <= C)
    data <- data.frame(X = X, Delta = Delta, Z1 = Z1, Z2 = Z2, W1 = W1, W2 = W2)
    
    cox_model <- coxph(Surv(X, Delta) ~ Z1 + Z2 + W1 + W2, data = data, na.action = na.omit)
    
    beta_estimates[i, ] <- cox_model$coefficients
    se_estimates[i, ] <- sqrt(diag(vcov(cox_model)))
  }
  
  beta_means <- colMeans(beta_estimates, na.rm = TRUE)
  beta_sds <- apply(beta_estimates, 2, sd, na.rm = TRUE)
  se_means <- colMeans(se_estimates, na.rm = TRUE)
  se_sds <- apply(se_estimates, 2, sd, na.rm = TRUE)
  
  return(list(
    beta_means = beta_means,
    beta_sds = beta_sds,
    se_means = se_means,
    se_sds = se_sds
  ))
}

# Ensure T is precomputed
lambda_t <- exp(beta1 * Z1 + beta2 * W1 + beta3 * W2)
u <- runif(n, 0, 1)
T <- (-log(1 - u) / (lambda_t * rho))^(1 / gamma)

# Parallel simulation for each censoring rate
results_parallel <- mclapply(seq_along(censoring_rates), function(i) {
  rate <- censoring_rates[i]
  lambda_c <- lambda_c_values_large[i]
  
  simulate_with_censoring_fixed(
    censor_rate_target = rate,
    num_simulations = num_simulations,
    n = n,
    T = T,
    Z1 = Z1,
    Z2 = Z2,
    W1 = W1,
    W2 = W2,
    lambda_c = lambda_c
  )
}, mc.cores = detectCores())

# Convert results to proper list structure if necessary
results_parallel <- lapply(results_parallel, function(res) {
  if (!is.list(res)) {
    stop("Unexpected structure in results. Each result should be a list.")
  }
  res
})

# Summarize results
summary_results <- data.frame(
  CensoringRate = rep(censoring_rates, each = 4),
  Parameter = rep(c("Z1", "Z2", "W1", "W2"), times = length(censoring_rates)),
  BetaMean = unlist(lapply(results_parallel, function(x) x$beta_means)),
  BetaSD = unlist(lapply(results_parallel, function(x) x$beta_sds)),
  SEMean = unlist(lapply(results_parallel, function(x) x$se_means)),
  SESD = unlist(lapply(results_parallel, function(x) x$se_sds))
)

# Print results
print(summary_results)

```


### (d) Model Performance under Different Censoring Rates - n=10,000,000
```{r}

```

## Part 2.

### (a): Case-cohort design:
```{r}
# Censoring rate 0.99에 대한 데이터 생성 및 시뮬레이션
lambda_c_99 <- 15.92681178  # Censoring rate 0.99에 대응하는 lambda_c 값
num_simulations <- 2000
datasets_censor_0.99 <- list()
for (i in 1:num_simulations) {
  # Covariates Z1, Z2
  z_values <- mvrnorm(n = n, mu = c(0, 0), Sigma = matrix(c(1, 0.75, 0.75, 1), 2))
  Z1 <- z_values[, 1]
  Z2 <- z_values[, 2]

  # Covariates W1, W2
  W1 <- rnorm(n, mean = 0, sd = 1)
  W2 <- rbinom(n, size = 1, prob = 0.5)

  # Uniform random variable u for survival times
  u <- runif(n, min = 0, max = 1)

  # True survival times T
  lambda_t <- exp(log(2) * Z1 + log(2) * W1 - 1 * W2)  # Coefficients based on original settings
  T <- (-log(1 - u) / (lambda_t * 0.5))^(1 / 1.5)

  # Censoring times C
  C <- rexp(n, rate = lambda_c_99)

  # Observed times and censoring indicator
  X <- pmin(T, C)
  Delta <- as.numeric(T <= C)

  # Dataframe creation
  datasets_censor_0.99[[i]] <- data.frame(X = X, Delta = Delta, Z1 = Z1, Z2 = Z2, W1 = W1, W2 = W2)
}
```

```{r}
simulate_case_cohort <- function(datasets, subcohort_size) {
  beta_estimates <- matrix(NA, nrow = length(datasets), ncol = 3)  # For β1, β2, β3
  subcohort_sizes <- numeric(length(datasets))  # 실제 서브코호트 크기를 저장
  failures_counts <- numeric(length(datasets))  # 실패 샘플의 수를 저장

  for (i in 1:length(datasets)) {
    data <- datasets[[i]]

    # Step 1: Identify failures (Delta == 1) and non-failures
    failures <- data[data$Delta == 1, ]
    non_failures <- data[data$Delta == 0, ]

    # Step 2: Sample subcohort
    sampled_non_failures <- non_failures[sample(1:nrow(non_failures), subcohort_size, replace = FALSE), ]

    # Combine failures and sampled non-failures
    subcohort <- rbind(failures, sampled_non_failures)

    # Calculate weights
    n_c <- nrow(non_failures)  # Total non-failures in the full cohort
    n_c_tilde <- nrow(sampled_non_failures)  # Sampled non-failures in subcohort
    subcohort$weights <- ifelse(subcohort$Delta == 1, 1, n_c / n_c_tilde)  # Assign weights

    # Step 3: Fit Cox model with weights
    cox_model <- coxph(Surv(X, Delta) ~ Z1 + W1 + W2, data = subcohort, weights = subcohort$weights)
    beta_estimates[i, ] <- cox_model$coefficients  # Save beta coefficients

    # Save additional statistics
    subcohort_sizes[i] <- nrow(subcohort)
    failures_counts[i] <- nrow(failures)
  }

  # Calculate summary statistics
  beta_means <- colMeans(beta_estimates, na.rm = TRUE)
  beta_sds <- apply(beta_estimates, 2, sd, na.rm = TRUE)
  avg_failures <- mean(failures_counts)
  avg_subcohort_size <- mean(subcohort_sizes)

  return(list(
    beta_means = beta_means,
    beta_sds = beta_sds,
    avg_failures = avg_failures,
    avg_subcohort_size = avg_subcohort_size
  ))
}
```


#### 시뮬레이션 실행 (subcohort_size = 100)
```{r}
cat("Running simulation for Censoring Rate: 0.99\n")
simulation_results_99_100 <- simulate_case_cohort(datasets_censor_0.99, subcohort_size = 100)

# 결과를 데이터프레임으로 정리
results_summary_99_100 <- data.frame(
  Parameter = c("Beta1", "Beta2", "Beta3"),
  Mean_Estimate = simulation_results_99_100$beta_means,
  SD_Estimate = simulation_results_99_100$beta_sds,
  Avg_Failures = simulation_results_99_100$avg_failures,
  Avg_SubcohortSize = simulation_results_99_100$avg_subcohort_size
)

# 결과 출력
print(results_summary_99_100)
```

#### 시뮬레이션 실행 (subcohort_size = 300)
```{r}
cat("Running simulation for Censoring Rate: 0.99\n")
simulation_results_99_300 <- simulate_case_cohort(datasets_censor_0.99, subcohort_size = 300)

# 결과를 데이터프레임으로 정리
results_summary_99_300 <- data.frame(
  Parameter = c("Beta1", "Beta2", "Beta3"),
  Mean_Estimate = simulation_results_99_300$beta_means,
  SD_Estimate = simulation_results_99_300$beta_sds,
  Avg_Failures = simulation_results_99_300$avg_failures,
  Avg_SubcohortSize = simulation_results_99_300$avg_subcohort_size
)

# 결과 출력
print(results_summary_99_300)
```

#### 시뮬레이션 실행 (subcohort_size = 700)
```{r}
cat("Running simulation for Censoring Rate: 0.99\n")
simulation_results_99_700 <- simulate_case_cohort(datasets_censor_0.99, subcohort_size = 700)

# 결과를 데이터프레임으로 정리
results_summary_99_700 <- data.frame(
  Parameter = c("Beta1", "Beta2", "Beta3"),
  Mean_Estimate = simulation_results_99_700$beta_means,
  SD_Estimate = simulation_results_99_700$beta_sds,
  Avg_Failures = simulation_results_99_700$avg_failures,
  Avg_SubcohortSize = simulation_results_99_700$avg_subcohort_size
)

# 결과 출력
print(results_summary_99_700)
```

#### 시뮬레이션 실행 (subcohort_size = 1200)
```{r}
cat("Running simulation for Censoring Rate: 0.99\n")
simulation_results_99_1200 <- simulate_case_cohort(datasets_censor_0.99, subcohort_size = 1200)

# 결과를 데이터프레임으로 정리
results_summary_99_1200 <- data.frame(
  Parameter = c("Beta1", "Beta2", "Beta3"),
  Mean_Estimate = simulation_results_99_1200$beta_means,
  SD_Estimate = simulation_results_99_1200$beta_sds,
  Avg_Failures = simulation_results_99_1200$avg_failures,
  Avg_SubcohortSize = simulation_results_99_1200$avg_subcohort_size
)

# 결과 출력
print(results_summary_99_1200)
```

### (b): Nested case-cohort design:
```{r}
# Nested Case-Control Sampling
nested_case_control_sampling <- function(data, control_size = 1) {
  failures <- data[data$Delta == 1, ]
  non_failures <- data[data$Delta == 0, ]
  
  nested_data <- failures
  
  # Pre-sort non_failures by failure time to reduce filtering overhead
  non_failures <- non_failures[order(non_failures$X), ]
  
  for (i in 1:nrow(failures)) {
    failure_time <- failures$X[i]
    at_risk_controls <- non_failures[non_failures$X > failure_time, ]
    
    if (nrow(at_risk_controls) >= control_size) {
      sampled_controls <- at_risk_controls[sample(1:nrow(at_risk_controls), control_size, replace = FALSE), ]
    } else {
      sampled_controls <- at_risk_controls
    }
    nested_data <- rbind(nested_data, sampled_controls)
  }
  
  cox_model <- tryCatch({
    coxph(Surv(X, Delta) ~ Z1 + W1 + W2, data = nested_data)
  }, error = function(e) {
    message("Error in Cox model fitting: ", e)
    return(NULL)
  })
  
  return(list(
    cox_model = cox_model,
    nested_size = nrow(nested_data),
    failures_count = nrow(failures)
  ))
}

# Simulate Nested Case-Control in Parallel
simulate_nested_case_control_parallel <- function(datasets, control_size = 1, cores = 4) {
  # Create a cluster for parallel processing
  cl <- makeCluster(cores)
  
  # Load required library on each node
  clusterEvalQ(cl, library(survival))
  
  # Export all necessary variables and functions to the cluster
  clusterExport(cl, varlist = c("nested_case_control_sampling", "control_size"), envir = environment())
  
  # Run parallel computation
  results <- parLapply(cl, datasets, function(data) {
    nested_case_control_sampling(data, control_size = control_size)
  })
  
  # Stop the cluster
  stopCluster(cl)
  
  # Process results
  beta_estimates <- do.call(rbind, lapply(results, function(res) {
    if (!is.null(res$cox_model)) {
      res$cox_model$coefficients
    } else {
      c(NA, NA, NA)
    }
  }))
  nested_sizes <- sapply(results, function(res) res$nested_size)
  failures_counts <- sapply(results, function(res) res$failures_count)
  
  avg_failures <- mean(failures_counts, na.rm = TRUE)
  avg_nested_size <- mean(nested_sizes, na.rm = TRUE)
  beta_means <- colMeans(beta_estimates, na.rm = TRUE)
  beta_sds <- apply(beta_estimates, 2, sd, na.rm = TRUE)
  
  return(list(
    beta_means = beta_means,
    beta_sds = beta_sds,
    avg_failures = avg_failures,
    avg_nested_size = avg_nested_size
  ))
}

# 병렬 처리로 실행
cat("Running simulation for control size 1...\n")
results_control_1 <- simulate_nested_case_control_parallel(datasets_censor_0.99, control_size = 1, cores = 4)

cat("Running simulation for control size 5...\n")
results_control_5 <- simulate_nested_case_control_parallel(datasets_censor_0.99, control_size = 5, cores = 4)

# 결과 요약 데이터프레임
results_summary <- data.frame(
  ControlSize = c(1, 5),
  AvgFailures = c(results_control_1$avg_failures, results_control_5$avg_failures),
  AvgNestedSize = c(results_control_1$avg_nested_size, results_control_5$avg_nested_size),
  BetaMean_B1 = c(results_control_1$beta_means[1], results_control_5$beta_means[1]),
  BetaMean_B2 = c(results_control_1$beta_means[2], results_control_5$beta_means[2]),
  BetaMean_B3 = c(results_control_1$beta_means[3], results_control_5$beta_means[3]),
  BetaSD_B1 = c(results_control_1$beta_sds[1], results_control_5$beta_sds[1]),
  BetaSD_B2 = c(results_control_1$beta_sds[2], results_control_5$beta_sds[2]),
  BetaSD_B3 = c(results_control_1$beta_sds[3], results_control_5$beta_sds[3])
)
print(results_summary)
```

## Part 3-1: End-Point Sampling (Naive)
```{r}
# End-Point Sampling 함수
end_point_sampling_naive <- function(data, sample_size) {
  # Step 1: Identify failures and non-failures
  failures <- data[data$Delta == 1, ]
  non_failures <- data[data$Delta == 0, ]
  
  # Step 2: Sort non-failures by observed time in descending order
  non_failures <- non_failures[order(-non_failures$X), ]
  
  # Step 3: Select all failures and the top-k non-failures
  sampled_non_failures <- non_failures[1:min(sample_size, nrow(non_failures)), ]
  sampled_data <- rbind(failures, sampled_non_failures)
  
  return(sampled_data)
}

# End-Point Sampling Simulation 함수
simulate_end_point_naive <- function(datasets, sample_size) {
  beta_estimates <- matrix(NA, nrow = length(datasets), ncol = 3)  # For β1, β2, β3
  
  for (i in 1:length(datasets)) {
    data <- datasets[[i]]
    
    # Apply End-Point Sampling
    sampled_data <- end_point_sampling_naive(data, sample_size)
    
    # Fit Cox model
    cox_model <- coxph(Surv(X, Delta) ~ Z1 + W1 + W2, data = sampled_data)
    beta_estimates[i, ] <- cox_model$coefficients
  }
  
  # Calculate mean and standard deviation of beta estimates
  beta_means <- colMeans(beta_estimates, na.rm = TRUE)
  beta_sds <- apply(beta_estimates, 2, sd, na.rm = TRUE)
  
  return(list(beta_means = beta_means, beta_sds = beta_sds))
}
```

### 시뮬레이션 실행 (End-Point Sampling, sample_size = 1500)
```{r}
# 시뮬레이션 실행 (End-Point Sampling, sample_size = 1500)
cat("Running End-Point Sampling for Censoring Rate: 0.99 (Sample Size: 1529)\n")
simulation_results_end_point_1500 <- simulate_end_point_naive(datasets_censor_0.99, sample_size = 1529)

# 결과를 데이터프레임으로 정리
results_summary_end_point_1500 <- data.frame(
  Parameter = c("Beta1", "Beta2", "Beta3"),
  Mean_Estimate = simulation_results_end_point_1500$beta_means,
  SD_Estimate = simulation_results_end_point_1500$beta_sds
)

# 결과 출력
print(results_summary_end_point_1500)
```

## Part 3-2: End-Point Sampling (Weight)
```{r}
# p(y) 정의: 관측된 Y에 대한 증가 함수
# compute_py <- function(y) {
#   return(min(1, 0.7 * y**2))
# }

# compute_py <- function(y) {
#   return(0.7 * y**2)
# }

compute_py <- function(y) {
  return(min(1, y))
}

end_point_sampling <- function(data, sample_size) {
  # Step 1: Identify failures and non-failures
  failures <- data[data$Delta == 1, ]
  non_failures <- data[data$Delta == 0, ]
  
  # Step 2: Sort non-failures by observed time (descending order)
  non_failures <- non_failures[order(-non_failures$X), ]
  
  # Step 3: Apply p(y) to calculate sampling probabilities
  non_failures$p_y <- sapply(non_failures$X, compute_py)
  
  # Assign default values to failures for missing columns
  failures$p_y <- 1  # Failures have p_y = 1 since they're always sampled
  
  # Step 4: Sample non-failures with probability proportional to p(y)
  sampled_non_failures <- non_failures[sample(1:nrow(non_failures), 
                                              size = sample_size, 
                                              prob = non_failures$p_y, 
                                              replace = FALSE), ]
  
  # Combine failures and sampled non-failures
  sampled_data <- rbind(failures, sampled_non_failures)
  
  # Assign inverse probability weights
  sampled_data$weights <- ifelse(is.na(sampled_data$p_y), 1, 1 / sampled_data$p_y)
  
  return(sampled_data)
}


simulate_end_point <- function(datasets, sample_size) {
  beta_estimates <- matrix(NA, nrow = length(datasets), ncol = 3)  # For β1, β2, β3
  
  for (i in 1:length(datasets)) {
    data <- datasets[[i]]
    
    # Apply End-Point Sampling
    sampled_data <- end_point_sampling(data, sample_size)
    
    # Fit Cox model with weights
    cox_model <- coxph(Surv(X, Delta) ~ Z1 + W1 + W2, data = sampled_data, weights = sampled_data$weights)
    beta_estimates[i, ] <- cox_model$coefficients
  }
  
  # Calculate mean and standard deviation of beta estimates
  beta_means <- colMeans(beta_estimates, na.rm = TRUE)
  beta_sds <- apply(beta_estimates, 2, sd, na.rm = TRUE)
  
  return(list(beta_means = beta_means, beta_sds = beta_sds))
}
```

### 시뮬레이션 실행 (End-Point Sampling, sample_size = 1031)
```{r}
cat("Running End-Point Sampling for Censoring Rate: 0.99 (Sample Size: 1031)\n")
simulation_results_end_point_1000 <- simulate_end_point(datasets_censor_0.99, sample_size = 1031)

# 결과를 데이터프레임으로 정리
results_summary_end_point_1000 <- data.frame(
  Parameter = c("Beta1", "Beta2", "Beta3"),
  Mean_Estimate = simulation_results_end_point_1000$beta_means,
  SD_Estimate = simulation_results_end_point_1000$beta_sds
)

# 결과 출력
print(results_summary_end_point_1000)
```


### 시뮬레이션 실행 (End-Point Sampling, sample_size = 1529)
```{r}
cat("Running End-Point Sampling for Censoring Rate: 0.99 (Sample Size: 1529)\n")
simulation_results_end_point_1500 <- simulate_end_point(datasets_censor_0.99, sample_size = 1529)

# 결과를 데이터프레임으로 정리
results_summary_end_point_1500 <- data.frame(
  Parameter = c("Beta1", "Beta2", "Beta3"),
  Mean_Estimate = simulation_results_end_point_1500$beta_means,
  SD_Estimate = simulation_results_end_point_1500$beta_sds
)

# 결과 출력
print(results_summary_end_point_1500)
```

### p(y) 변경 $0.7y^2$
```{r}
compute_py <- function(y) {
  return(min(1, 0.7 * y**2))
}

cat("Running End-Point Sampling for Censoring Rate: 0.99 (Sample Size: 1529)\n")
simulation_results_end_point_1500_y2 <- simulate_end_point(datasets_censor_0.99, sample_size = 1529)

# 결과를 데이터프레임으로 정리
results_summary_end_point_1500_y2 <- data.frame(
  Parameter = c("Beta1", "Beta2", "Beta3"),
  Mean_Estimate = simulation_results_end_point_1500_y2$beta_means,
  SD_Estimate = simulation_results_end_point_1500_y2$beta_sds
)

# 결과 출력
print(results_summary_end_point_1500_y2)
```


